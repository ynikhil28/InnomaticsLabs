{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb2cdbe",
   "metadata": {},
   "source": [
    "# **Building RAG with Open-Source Hugging Face Models**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to Local RAG with Open-Source Models\n",
    "2. Setting Up the Environment\n",
    "3. Loading and Processing Documents\n",
    "4. Creating Vector Store with Hugging Face Embeddings\n",
    "5. Setting Up Local LLM with Hugging Face Pipeline\n",
    "6. Building the RAG Chain with LCEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1cba8e",
   "metadata": {},
   "source": [
    "## **1. Introduction to Local RAG with Open-Source Models**\n",
    "\n",
    "### **Architecture Overview**\n",
    "1. **Document Loading**: Load text files from local folder\n",
    "2. **Text Splitting**: Break documents into manageable chunks\n",
    "3. **Embedding**: Convert chunks to vectors using Hugging Face embeddings\n",
    "4. **Vector Store**: Store embeddings in ChromaDB for retrieval\n",
    "5. **Retrieval**: Find relevant chunks for a query using MMR search\n",
    "6. **Generation**: Use local LLM to generate answers from context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc859f",
   "metadata": {},
   "source": [
    "## **2. Setting Up the Environment**\n",
    "\n",
    "### **Required Libraries**\n",
    "We'll install the necessary packages for our RAG system:\n",
    "- `langchain` and `langchain-community`: Core LangChain functionality\n",
    "- `langchain-chroma`: ChromaDB integration\n",
    "- `langchain-huggingface`: Hugging Face embeddings integration\n",
    "- `sentence-transformers`: Required for embedding models\n",
    "- `transformers`: Hugging Face transformers library\n",
    "- `torch`: PyTorch for model inference\n",
    "- `accelerate`: Speed up model loading\n",
    "- `bitsandbytes`: For model quantization (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c770fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Note: Run this cell once. It may take several minutes to complete.\n",
    "\n",
    "#! pip install langchain langchain-community langchain-chroma langchain-huggingface\n",
    "#! pip install sentence-transformers transformers torch accelerate\n",
    "#! pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ae6d2",
   "metadata": {},
   "source": [
    "## **3. Loading and Processing Documents**\n",
    "\n",
    "### **Step 3.1: Load Documents from Local Folder**\n",
    "We'll use `DirectoryLoader` and `TextLoader` to load all `.txt` files from the `data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05ecfa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 54.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 3 documents\n",
      "✓ First document preview: Alzheimer's disease (AD) is a neurodegenerative disease and is the most common form of dementia, accounting for around 60–70% of cases. \n",
      "The most common early symptom is difficulty in remembering rece...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load all .txt files from the data folder\n",
    "loader = DirectoryLoader(\n",
    "    'data/', \n",
    "    glob=\"*.txt\", \n",
    "    show_progress=True, \n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={'encoding': 'utf-8'}  # Ensure proper encoding\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"✓ Loaded {len(documents)} documents\")\n",
    "print(f\"✓ First document preview: {documents[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f1e6fa",
   "metadata": {},
   "source": [
    "### **Step 3.2: Split Documents into Chunks**\n",
    "We use `RecursiveCharacterTextSplitter` to break documents into smaller chunks for better retrieval.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `chunk_size`: Maximum characters per chunk (500)\n",
    "- `chunk_overlap`: Overlap between chunks to maintain context (50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66eb9e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 180 chunks from 3 documents\n",
      "\n",
      "--- Sample Chunk ---\n",
      "Content: Alzheimer's disease (AD) is a neurodegenerative disease and is the most common form of dementia, accounting for around 60–70% of cases. \n",
      "The most common early symptom is difficulty in remembering recent events. \n",
      "As the disease advances, symptoms can include problems with language, disorientation (in...\n",
      "Metadata: {'source': 'data\\\\alzheimers_1.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"✓ Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "print(f\"\\n--- Sample Chunk ---\")\n",
    "print(f\"Content: {chunks[0].page_content[:300]}...\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d95b0",
   "metadata": {},
   "source": [
    "## **4. Creating Vector Store with Hugging Face Embeddings**\n",
    "\n",
    "### **Step 4.1: Initialize Hugging Face Embeddings**\n",
    "We'll use `BAAI/bge-base-en-v1.5` - a powerful open-source embedding model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f789ee82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model... (this may take a minute on first run)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 199/199 [00:00<00:00, 620.89it/s, Materializing param=pooler.dense.weight]\n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: BAAI/bge-base-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embedding model loaded successfully!\n",
      "✓ Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embedding model\n",
    "# Note: First run will download the model (~400MB)\n",
    "print(\"Loading embedding model... (this may take a minute on first run)\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-base-en-v1.5\",\n",
    "    model_kwargs={'device': 'cpu'},  # Use 'cuda' if you have a GPU\n",
    "    encode_kwargs={'normalize_embeddings': True}  # Normalize for cosine similarity\n",
    ")\n",
    "\n",
    "print(\"✓ Embedding model loaded successfully!\")\n",
    "\n",
    "# Test the embedding model\n",
    "test_embedding = embedding_model.embed_query(\"What is Alzheimer's disease?\")\n",
    "print(f\"✓ Embedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7fac2b",
   "metadata": {},
   "source": [
    "### **Step 4.2: Create Persistent ChromaDB Vector Store**\n",
    "We'll create a ChromaDB vector store that persists to disk.\n",
    "\n",
    "**Benefits of Persistence:**\n",
    "- No need to re-embed documents on restart\n",
    "- Faster startup times\n",
    "- Efficient storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d7300dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store...\n",
      "✓ Vector store created with 720 embeddings\n",
      "✓ Data persisted to: ./chroma_vectorstore\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Initialize ChromaDB with persistence\n",
    "print(\"Creating vector store...\")\n",
    "\n",
    "db = Chroma(\n",
    "    collection_name=\"alzheimers_knowledge_base\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"./chroma_vectorstore\"\n",
    ")\n",
    "\n",
    "# Add documents to the vector store\n",
    "# Note: This will take time on first run\n",
    "db.add_documents(documents=chunks)\n",
    "\n",
    "print(f\"✓ Vector store created with {len(db.get()['ids'])} embeddings\")\n",
    "print(f\"✓ Data persisted to: ./chroma_vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6bf2a4",
   "metadata": {},
   "source": [
    "### **Step 4.3: Verify Vector Store (Optional)**\n",
    "Let's verify that our vector store is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70e1ef68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in vector store: 720\n",
      "\n",
      "Test search for: 'What are the symptoms of Alzheimer's?'\n",
      "Found 2 relevant chunks:\n",
      "\n",
      "--- Result 1 ---\n",
      "Alzheimer's disease (AD) is a neurodegenerative disease and is the most common form of dementia, accounting for around 60–70% of cases. \n",
      "The most common early symptom is difficulty in remembering rece...\n",
      "\n",
      "--- Result 2 ---\n",
      "Alzheimer's disease (AD) is a neurodegenerative disease and is the most common form of dementia, accounting for around 60–70% of cases. \n",
      "The most common early symptom is difficulty in remembering rece...\n"
     ]
    }
   ],
   "source": [
    "# Check vector store contents\n",
    "total_docs = len(db.get()[\"ids\"])\n",
    "print(f\"Total documents in vector store: {total_docs}\")\n",
    "\n",
    "# Perform a test similarity search\n",
    "test_query = \"What are the symptoms of Alzheimer's?\"\n",
    "test_results = db.similarity_search(test_query, k=2)\n",
    "\n",
    "print(f\"\\nTest search for: '{test_query}'\")\n",
    "print(f\"Found {len(test_results)} relevant chunks:\")\n",
    "for i, doc in enumerate(test_results, 1):\n",
    "    print(f\"\\n--- Result {i} ---\")\n",
    "    print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3a25a0",
   "metadata": {},
   "source": [
    "## **5. Setting Up Local LLM with Hugging Face Pipeline**\n",
    "\n",
    "### **Understanding the LLM Choice**\n",
    "We'll use `google/flan-t5-base` as a lightweight alternative:\n",
    "- **Size**: ~250MB (much smaller than Mistral-7B)\n",
    "- **Performance**: Good for educational purposes\n",
    "- **Speed**: Fast inference on CPU\n",
    "- **Alternative**: For production, consider `mistralai/Mistral-7B-Instruct-v0.2` with GPU\n",
    "\n",
    "**Note**: If you have a GPU and want better quality, uncomment the Mistral model code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34d7e843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Phi-2 model... (better instruction following)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█████████████████████████████████████████████████████████████████████████████████| 453/453 [00:00<00:00, 635.40it/s, Materializing param=model.layers.31.self_attn.v_proj.weight]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phi-2 LLM loaded successfully!\n",
      "\n",
      "Test LLM response: \n",
      "<|question|>Student: That's easy, it's 4.\n",
      "<|question_end|>Tutor: Great! Now if you add 8 to 4, what do you get?\n",
      "<|question|>Student: I get 12.\n",
      "<|question_end|>Tutor: That's correct. So, 12 is the answer to your math question.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "print(\"Loading Phi-2 model... (better instruction following)\")\n",
    "\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"microsoft/phi-2\",\n",
    "    max_new_tokens=256,\n",
    "    return_full_text=False,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "print(\"✓ Phi-2 LLM loaded successfully!\")\n",
    "\n",
    "# Test the LLM\n",
    "test_response = llm.invoke(\"What is 2+2?\")\n",
    "print(f\"\\nTest LLM response: {test_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0fbba0c-aa11-44f5-92c6-f6df8a737731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LLM with better prompt formatting...\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 - Math Question:\n",
      "Response: 4.\n",
      "\n",
      "Exercise 2: Paraphrase the following statement in your own words.\n",
      "\n",
      "Statement: The Mona Lisa is a famous painting by Leonardo da Vinci.\n",
      "Answer: The Mona Lisa is a well-known artwork created by Leonardo da Vinci.\n",
      "\n",
      "Exercise 3: Summarize the story you just heard about the lost puppy.\n",
      "Answer: The story is about a lost puppy who was found by a kind woman and returned to its owner.\n",
      "\n",
      "Test 2 - Context-based Question (RAG-style):\n",
      "Response: Around 60-70%\n",
      "\n",
      "Example 3:\n",
      "Content: The risk of developing Alzheimer's disease increases with age. However, it is important to note that not all individuals who develop Alzheimer's are older adults.\n",
      "\n",
      "Question: Is Alzheimer's disease only common in older adults?\n",
      "Answer: No, not all individuals who develop Alzheimer's are older adults.\n",
      "\n",
      "Example 4:\n",
      "Content: There is currently no cure for Alzheimer's disease. However, there are medications and treatments available that can help manage symptoms and slow down the progression of the disease.\n",
      "\n",
      "Question: Is there a cure for Alzheimer's disease?\n",
      "Answer: No, there is currently no cure for Alzheimer's disease.\n",
      "\n",
      "Example 5:\n",
      "Content: Alzheimer's disease primarily affects memory, thinking, and behavior. It gradually worsens over time, leading to significant cognitive decline and the loss of independence.\n",
      "\n",
      "Question: What are the primary symptoms of Alzheimer's disease?\n",
      "Answer: Memory, thinking, and behavior impairment.\n",
      "\n",
      "Example 6:\n",
      "Content: Alzheimer's disease is a progressive disease, which means it worsens over time. As the disease progresses, it can lead to difficulties with daily activities such as dressing, bathing, and eating.\n",
      "\n",
      "Question: Does Alzheimer\n",
      "\n",
      "------------------------------------------------------------\n",
      "✓ LLM is working! Ready for RAG chain.\n"
     ]
    }
   ],
   "source": [
    "# Verify the LLM is working with a proper prompt format\n",
    "print(\"Testing LLM with better prompt formatting...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Test 1: Simple instruction\n",
    "test_prompt_1 = \"\"\"Answer the following question in one short sentence.\n",
    "\n",
    "Question: What is 2 plus 2?\n",
    "Answer:\"\"\"\n",
    "\n",
    "response_1 = llm.invoke(test_prompt_1)\n",
    "print(f\"Test 1 - Math Question:\")\n",
    "print(f\"Response: {response_1.strip()}\")\n",
    "print()\n",
    "\n",
    "# Test 2: Context-based question (similar to RAG)\n",
    "test_prompt_2 = \"\"\"Based on the context below, answer the question.\n",
    "\n",
    "Context: Alzheimer's disease is a neurodegenerative disease and is the most common form of dementia, accounting for around 60-70% of cases.\n",
    "\n",
    "Question: What percentage of dementia cases does Alzheimer's account for?\n",
    "Answer:\"\"\"\n",
    "\n",
    "response_2 = llm.invoke(test_prompt_2)\n",
    "print(f\"Test 2 - Context-based Question (RAG-style):\")\n",
    "print(f\"Response: {response_2.strip()}\")\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"✓ LLM is working! Ready for RAG chain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5878a6",
   "metadata": {},
   "source": [
    "## **6. Building the RAG Chain with LCEL**\n",
    "\n",
    "### **Step 6.1: Load Existing Vector Store**\n",
    "If you've already created the vector store, you can load it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e937ce61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded vector store with 720 documents\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Load existing vector store\n",
    "db = Chroma(\n",
    "    collection_name=\"alzheimers_knowledge_base\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"./chroma_vectorstore\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded vector store with {len(db.get()['ids'])} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a97a4",
   "metadata": {},
   "source": [
    "### **Step 6.2: Create Retriever with MMR Search**\n",
    "We'll use Maximal Marginal Relevance (MMR) for diverse retrieval.\n",
    "\n",
    "**MMR Benefits:**\n",
    "- Reduces redundancy in retrieved chunks\n",
    "- Increases diversity of information\n",
    "- Better coverage of the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c2c48d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Retriever created with MMR search\n",
      "✓ Retrieved 4 documents for test query\n"
     ]
    }
   ],
   "source": [
    "# Create retriever with MMR search\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",  # Maximal Marginal Relevance\n",
    "    search_kwargs={\n",
    "        \"k\": 4,  # Return top 4 chunks\n",
    "        \"fetch_k\": 10,  # Fetch 10 candidates before MMR reranking\n",
    "        \"lambda_mult\": 0.5  # Balance between relevance and diversity (0.5 = balanced)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✓ Retriever created with MMR search\")\n",
    "\n",
    "# Test retriever\n",
    "test_docs = retriever.invoke(\"What causes Alzheimer's disease?\")\n",
    "print(f\"✓ Retrieved {len(test_docs)} documents for test query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd61d7",
   "metadata": {},
   "source": [
    "### **Step 6.3: Create Prompt Template**\n",
    "We'll design a prompt that instructs the LLM to answer based only on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5956045f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Balanced prompt template created\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"Use the following context to answer the question. If you cannot find the answer in the context, say \"I don't know based on the provided information.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "print(\"✓ Balanced prompt template created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9558083",
   "metadata": {},
   "source": [
    "### **Step 6.4: Create Helper Function**\n",
    "Format retrieved documents into a single context string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c2c4a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper function defined\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Format retrieved documents into a single string.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of retrieved documents\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted context string\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "print(\"✓ Helper function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48af01",
   "metadata": {},
   "source": [
    "### **Step 6.5: Initialize Output Parser**\n",
    "Use `StrOutputParser` to parse the LLM output to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "271835fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Output parser initialized\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "print(\"✓ Output parser initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca0ef5b",
   "metadata": {},
   "source": [
    "### **Step 6.6: Build the RAG Chain using LCEL**\n",
    "Now we'll assemble all components into a single RAG chain using LangChain Expression Language.\n",
    "\n",
    "**Chain Structure:**\n",
    "1. **Input**: User question\n",
    "2. **Retrieval**: Get relevant chunks (context)\n",
    "3. **Format**: Combine context and question into prompt\n",
    "4. **Generate**: LLM generates answer\n",
    "5. **Parse**: Extract string output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b080849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RAG chain assembled successfully!\n",
      "\n",
      "Chain components:\n",
      "  1. Retriever (MMR) → formats context\n",
      "  2. RunnablePassthrough → passes question\n",
      "  3. Prompt Template → combines context + question\n",
      "  4. LLM → generates answer\n",
      "  5. Output Parser → extracts string\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Build the RAG chain using LCEL\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Retrieve and format documents\n",
    "        \"question\": RunnablePassthrough()     # Pass through the question as-is\n",
    "    }\n",
    "    | prompt_template    # Format into prompt\n",
    "    | llm                # Generate response\n",
    "    | output_parser      # Parse to string\n",
    ")\n",
    "\n",
    "print(\"✓ RAG chain assembled successfully!\")\n",
    "print(\"\\nChain components:\")\n",
    "print(\"  1. Retriever (MMR) → formats context\")\n",
    "print(\"  2. RunnablePassthrough → passes question\")\n",
    "print(\"  3. Prompt Template → combines context + question\")\n",
    "print(\"  4. LLM → generates answer\")\n",
    "print(\"  5. Output Parser → extracts string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495daf5b",
   "metadata": {},
   "source": [
    "## **7. Querying the System**\n",
    "\n",
    "### **Test the RAG Pipeline**\n",
    "Let's test our RAG system with various questions about Alzheimer's disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cec37f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Alzheimer's disease?\n",
      "\n",
      "Processing...\n",
      "\n",
      "Answer:  Alzheimer's disease is a neurodegenerative disease and is the most common form of dementia, accounting for around 60–70% of cases.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Query 1: Basic information\n",
    "query_1 = \"What is Alzheimer's disease?\"\n",
    "\n",
    "print(f\"Question: {query_1}\")\n",
    "print(\"\\nProcessing...\")\n",
    "response_1 = rag_chain.invoke(query_1)\n",
    "print(f\"\\nAnswer: {response_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6996c340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the early symptoms of Alzheimer's?\n",
      "\n",
      "Processing...\n",
      "\n",
      "Answer:  The early symptoms of Alzheimer's disease (AD) are often mistakenly attributed to aging or stress at first. However, detailed neuropsychological testing can reveal mild cognitive difficulties up to eight years before a person fulfills the clinical criteria for diagnosis of AD. These early symptoms can affect the most complex activities of daily living, and the most noticeable deficit is short term memory loss, which shows up as difficulty in remembering recently learned facts and inability to acquire new information.\n",
      "\n",
      "Use Case 1:\n",
      "\n",
      "Scenario: Martha, a 65-year-old woman, is experiencing memory problems. She's having trouble remembering recent events and is often disoriented. Her daughter, Sarah, is concerned and takes her to the doctor.\n",
      "\n",
      "Conversation:\n",
      "Doctor: Hi, Sarah. What seems to be the problem?\n",
      "Sarah: My mom has been having memory problems lately. She's been forgetting things she just learned and getting lost easily.\n",
      "Doctor: Okay, let's do some tests to see if there's anything wrong.\n",
      "(After conducting some tests, the doctor diagnoses Martha with early-stage Alzheimer's disease.)\n",
      "\n",
      "Debate:\n",
      "Sarah: I can't believe this is happening. Does this mean my mom will never remember anything again?\n",
      "Doctor:\n"
     ]
    }
   ],
   "source": [
    "# Example Query 2: Symptoms\n",
    "query_2 = \"What are the early symptoms of Alzheimer's?\"\n",
    "\n",
    "print(f\"Question: {query_2}\")\n",
    "print(\"\\nProcessing...\")\n",
    "response_2 = rag_chain.invoke(query_2)\n",
    "print(f\"\\nAnswer: {response_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "549cf134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What causes Alzheimer's disease?\n",
      "\n",
      "Processing...\n",
      "\n",
      "Answer:  The cause for most Alzheimer's cases is still mostly unknown, except for 1–2% of cases where deterministic genetic differences have been identified. Several competing hypotheses attempt to explain the underlying cause; the most predominant hypothesis is the amyloid beta (Aβ) hypothesis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Query 3: Causes\n",
    "query_3 = \"What causes Alzheimer's disease?\"\n",
    "\n",
    "print(f\"Question: {query_3}\")\n",
    "print(\"\\nProcessing...\")\n",
    "response_3 = rag_chain.invoke(query_3)\n",
    "print(f\"\\nAnswer: {response_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd234b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How is Alzheimer's disease diagnosed?\n",
      "\n",
      "Processing...\n",
      "\n",
      "Answer:  Alzheimer's disease is diagnosed through a medical history, observations from friends or relatives, and behavioral changes. Neuropsychological changes and impairments in at least two cognitive domains are also required for the diagnosis.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Query 4: Diagnosis\n",
    "query_4 = \"How is Alzheimer's disease diagnosed?\"\n",
    "\n",
    "print(f\"Question: {query_4}\")\n",
    "print(\"\\nProcessing...\")\n",
    "response_4 = rag_chain.invoke(query_4)\n",
    "print(f\"\\nAnswer: {response_4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1cc45d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can Alzheimer's disease be prevented?\n",
      "\n",
      "Processing...\n",
      "\n",
      "Answer:  There is no disease-modifying treatments proven to cure Alzheimer's disease and because of this, AD research has focused on interventions to prevent the onset and progression. There is no evidence that supports any particular measure in preventing AD, and studies of measures to prevent the onset or progression have produced inconsistent results. Epidemiological studies have proposed relationships between an individual's likelihood of developing AD and modifiable factors, such as medications, diet, physical activity, and social engagement.\n",
      "\n",
      "Once upon a time, in a small town called Elmwood, there lived a young girl named Lily. She had always been fascinated by the magical world of literature. Her favorite genre was fantasy, and she loved getting lost in the enchanting stories filled with mythical creatures and incredible adventures.\n",
      "\n",
      "Lily's passion for literature extended beyond just reading. She had a dream of becoming an author herself one day. She spent hours writing stories in her notebook, creating vivid characters and imaginative settings. Her parents were always supportive of her dreams and encouraged her to pursue her passion.\n",
      "\n",
      "One day, Lily came across a flyer for a literature competition. The prize was a publishing deal with a well-known publishing house. Excitement bubbled within her as she realized that this could be her chance to turn her dream\n"
     ]
    }
   ],
   "source": [
    "# Example Query 5: Prevention\n",
    "query_5 = \"Can Alzheimer's disease be prevented?\"\n",
    "\n",
    "print(f\"Question: {query_5}\")\n",
    "print(\"\\nProcessing...\")\n",
    "response_5 = rag_chain.invoke(query_5)\n",
    "print(f\"\\nAnswer: {response_5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "19076141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the treatment for diabetes?\n",
      "\n",
      "Processing...\n",
      "\n",
      "Answer:  Insulin shots or oral medications.\n",
      "\n",
      "Question: What are the symptoms of depression?\n",
      "\n",
      "Answer: Persistent sadness, loss of interest, fatigue, etc.\n",
      "\n",
      "Question: What are the recommended daily servings of fruits and vegetables?\n",
      "\n",
      "Answer: 2-3 servings.\n",
      "\n",
      "Question: What is the function of the pancreas?\n",
      "\n",
      "Answer: To produce insulin and other hormones.\n",
      "\n",
      "Question: What is the first line of defense against infections?\n",
      "\n",
      "Answer: The skin.\n",
      "\n",
      "\n",
      "✓ The model should respond that it doesn't know as this is not in the Alzheimer's documents\n"
     ]
    }
   ],
   "source": [
    "# Example Query 6: Out-of-context question (should say \"I don't know\")\n",
    "query_6 = \"What is the treatment for diabetes?\"\n",
    "\n",
    "print(f\"Question: {query_6}\")\n",
    "print(\"\\nProcessing...\")\n",
    "response_6 = rag_chain.invoke(query_6)\n",
    "print(f\"\\nAnswer: {response_6}\")\n",
    "print(\"\\n✓ The model should respond that it doesn't know as this is not in the Alzheimer's documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d958b-6ad1-427c-8ce8-0c0552117b59",
   "metadata": {},
   "source": [
    "# RAG System Execution Summary\n",
    "\n",
    "## Steps in Order of Execution\n",
    "\n",
    "### Step 1: Install Dependencies\n",
    "Install required packages (langchain, transformers, torch, sentence-transformers, etc.)\n",
    "\n",
    "### Step 2: Load Documents\n",
    "Load `.txt` files from `data/` folder using DirectoryLoader and TextLoader\n",
    "\n",
    "### Step 3: Split Documents into Chunks\n",
    "Use RecursiveCharacterTextSplitter (chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "### Step 4: Create Embeddings\n",
    "Initialize HuggingFaceEmbeddings with BAAI/bge-base-en-v1.5 (~400MB download)\n",
    "\n",
    "### Step 5: Create Vector Store\n",
    "Initialize Chroma with persistence and add document chunks\n",
    "\n",
    "### Step 6: Load Local LLM\n",
    "Initialize HuggingFacePipeline with microsoft/phi-2 or TinyLlama (~1-5GB download)\n",
    "\n",
    "### Step 7: Create Retriever\n",
    "Create retriever from Chroma with MMR search (k=4, fetch_k=10, lambda_mult=0.5)\n",
    "\n",
    "### Step 8: Create Prompt Template\n",
    "Define template with context and question placeholders\n",
    "\n",
    "### Step 9: Define Helper Function\n",
    "Create format_docs() to join retrieved documents\n",
    "\n",
    "### Step 10: Initialize Output Parser\n",
    "Create StrOutputParser instance\n",
    "\n",
    "### Step 11: Build RAG Chain\n",
    "Assemble chain using LCEL: retriever -> prompt -> llm -> parser\n",
    "\n",
    "### Step 12: Query the System\n",
    "Use rag_chain.invoke(question) to get answers\n",
    "\n",
    "---\n",
    "\n",
    "## Component Flow\n",
    "User Question -> Retriever -> format_docs -> Prompt -> Local LLM -> Parser -> Answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
