{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665ab62d",
   "metadata": {},
   "source": [
    "# 1. **Building RAG with Cloud-Based OpenAI Models**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to Cloud RAG with OpenAI\n",
    "2. Setting Up the Environment\n",
    "3. Loading and Processing Documents\n",
    "4. Creating Vector Store with Hugging Face Embeddings (Local)\n",
    "5. Setting Up OpenAI Cloud LLM\n",
    "6. Building the RAG Chain with LCEL\n",
    "7. Querying the System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e9d6a",
   "metadata": {},
   "source": [
    "## **2. Setting Up the Environment**\n",
    "\n",
    "### **Required Libraries**\n",
    "We'll install the necessary packages for our RAG system:\n",
    "- `langchain` and `langchain-community`: Core LangChain functionality\n",
    "- `langchain-chroma`: ChromaDB integration\n",
    "- `langchain-huggingface`: Hugging Face embeddings integration (local)\n",
    "- `langchain-openai`: OpenAI integration for cloud LLM\n",
    "- `sentence-transformers`: Required for embedding models\n",
    "- `chromadb`: Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce1f1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-1.1.10-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.13 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from langchain-openai) (1.2.13)\n",
      "Collecting openai<3.0.0,>=2.20.0 (from langchain-openai)\n",
      "  Downloading openai-2.24.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken<1.0.0,>=0.7.0 (from langchain-openai)\n",
      "  Using cached tiktoken-0.12.0-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (0.7.3)\n",
      "Requirement already satisfied: packaging>=23.2.0 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (26.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (9.1.4)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0 (from openai<3.0.0,>=2.20.0->langchain-openai)\n",
      "  Using cached jiter-0.13.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting sniffio (from openai<3.0.0,>=2.20.0->langchain-openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (4.67.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2026.2.19)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.5)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=2.20.0->langchain-openai) (3.11)\n",
      "Requirement already satisfied: certifi in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=2.20.0->langchain-openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=2.20.0->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=2.20.0->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.13->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-openai) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: xxhash>=3.0.0 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-openai) (3.6.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-openai) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain-openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain-openai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.6.3)\n",
      "Requirement already satisfied: colorama in d:\\mydata\\training and ks\\ai solution enablement program\\handsonlabs\\lang-chain-expirement\\.venv-langchain\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=2.20.0->langchain-openai) (0.4.6)\n",
      "Downloading langchain_openai-1.1.10-py3-none-any.whl (87 kB)\n",
      "   ---------------------------------------- 0.0/87.2 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 81.9/87.2 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 87.2/87.2 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading openai-2.24.0-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.2/1.1 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.3/1.1 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 0.9/1.1 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.1/1.1 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 5.5 MB/s eta 0:00:00\n",
      "Using cached tiktoken-0.12.0-cp312-cp312-win_amd64.whl (878 kB)\n",
      "Using cached jiter-0.13.0-cp312-cp312-win_amd64.whl (205 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sniffio, jiter, tiktoken, openai, langchain-openai\n",
      "Successfully installed jiter-0.13.0 langchain-openai-1.1.10 openai-2.24.0 sniffio-1.3.1 tiktoken-0.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Note: Run this cell once. It may take several minutes to complete.\n",
    "\n",
    "# ! pip install langchain langchain-community langchain-chroma langchain-huggingface langchain-openai\n",
    "# ! pip install sentence-transformers chromadb\n",
    "#! pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2930caf",
   "metadata": {},
   "source": [
    "## **3. Loading and Processing Documents**\n",
    "\n",
    "### **Step 3.1: Load Documents from Local Folder**\n",
    "We'll use `DirectoryLoader` and `TextLoader` to load all `.txt` files from the `data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62556d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MyData\\Training and KS\\AI Solution Enablement Program\\HandsOnLabs\\lang-chain-expirement\\.venv-langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 58.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 3 documents\n",
      "✓ First document preview: Alzheimer's disease (AD) is a neurodegenerative disease and is the most common form of dementia, accounting for around 60–70% of cases. \n",
      "The most common early symptom is difficulty in remembering rece...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load all .txt files from the data folder\n",
    "loader = DirectoryLoader(\n",
    "    'data/', \n",
    "    glob=\"*.txt\", \n",
    "    show_progress=True, \n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={'encoding': 'utf-8'}  # Ensure proper encoding\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"✓ Loaded {len(documents)} documents\")\n",
    "print(f\"✓ First document preview: {documents[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fa2cf",
   "metadata": {},
   "source": [
    "### **Step 3.2: Split Documents into Chunks**\n",
    "We use `RecursiveCharacterTextSplitter` to break documents into smaller chunks for better retrieval.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `chunk_size`: Maximum characters per chunk (500)\n",
    "- `chunk_overlap`: Overlap between chunks to maintain context (50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3985955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 180 chunks from 3 documents\n",
      "\n",
      "--- Sample Chunk ---\n",
      "Content: Alzheimer's disease (AD) is a neurodegenerative disease and is the most common form of dementia, accounting for around 60–70% of cases. \n",
      "The most common early symptom is difficulty in remembering recent events. \n",
      "As the disease advances, symptoms can include problems with language, disorientation (in...\n",
      "Metadata: {'source': 'data\\\\alzheimers_1.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"✓ Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "print(f\"\\n--- Sample Chunk ---\")\n",
    "print(f\"Content: {chunks[0].page_content[:300]}...\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695088c",
   "metadata": {},
   "source": [
    "## **4. Creating Vector Store with Hugging Face Embeddings**\n",
    "\n",
    "### **Step 4.1: Initialize Hugging Face Embeddings**\n",
    "We'll use `BAAI/bge-base-en-v1.5` - a powerful open-source embedding model (local).\n",
    "\n",
    "**Note**: We keep embeddings local because:\n",
    "- Cost-effective (free after download)\n",
    "- Fast for batch processing\n",
    "- Privacy-friendly\n",
    "- OpenAI embeddings can be expensive for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "790db7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model... (this may take a minute on first run)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 199/199 [00:00<00:00, 599.98it/s, Materializing param=pooler.dense.weight]\n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: BAAI/bge-base-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embedding model loaded successfully!\n",
      "✓ Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embedding model\n",
    "# Note: First run will download the model (~400MB)\n",
    "print(\"Loading embedding model... (this may take a minute on first run)\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-base-en-v1.5\",\n",
    "    model_kwargs={'device': 'cpu'},  # Use 'cuda' if you have a GPU\n",
    "    encode_kwargs={'normalize_embeddings': True}  # Normalize for cosine similarity\n",
    ")\n",
    "\n",
    "print(\"✓ Embedding model loaded successfully!\")\n",
    "\n",
    "# Test the embedding model\n",
    "test_embedding = embedding_model.embed_query(\"What is Alzheimer's disease?\")\n",
    "print(f\"✓ Embedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b662c",
   "metadata": {},
   "source": [
    "### **Step 4.2: Create Persistent ChromaDB Vector Store**\n",
    "We'll create a ChromaDB vector store that persists to disk.\n",
    "\n",
    "**Benefits of Persistence:**\n",
    "- No need to re-embed documents on restart\n",
    "- Faster startup times\n",
    "- Efficient storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "957cd7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store...\n",
      "✓ Vector store created with 900 embeddings\n",
      "✓ Data persisted to: ./chroma_vectorstore\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Initialize ChromaDB with persistence\n",
    "print(\"Creating vector store...\")\n",
    "\n",
    "db = Chroma(\n",
    "    collection_name=\"alzheimers_knowledge_base\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"./chroma_vectorstore\"\n",
    ")\n",
    "\n",
    "# Add documents to the vector store\n",
    "# Note: This will take time on first run\n",
    "db.add_documents(documents=chunks)\n",
    "\n",
    "print(f\"✓ Vector store created with {len(db.get()['ids'])} embeddings\")\n",
    "print(f\"✓ Data persisted to: ./chroma_vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf2783",
   "metadata": {},
   "source": [
    "### **Step 4.3: Verify Vector Store (Optional)**\n",
    "Let's verify that our vector store is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc4430e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in vector store: 900\n",
      "\n",
      "Test search for: 'What are the symptoms of Alzheimer's?'\n",
      "Found 2 relevant chunks:\n",
      "\n",
      "--- Result 1 ---\n",
      "Alzheimer's disease (AD) is a neurodegenerative disease and is the most common form of dementia, accounting for around 60–70% of cases. \n",
      "The most common early symptom is difficulty in remembering rece...\n",
      "\n",
      "--- Result 2 ---\n",
      "Alzheimer's disease (AD) is a neurodegenerative disease and is the most common form of dementia, accounting for around 60–70% of cases. \n",
      "The most common early symptom is difficulty in remembering rece...\n"
     ]
    }
   ],
   "source": [
    "# Check vector store contents\n",
    "total_docs = len(db.get()[\"ids\"])\n",
    "print(f\"Total documents in vector store: {total_docs}\")\n",
    "\n",
    "# Perform a test similarity search\n",
    "test_query = \"What are the symptoms of Alzheimer's?\"\n",
    "test_results = db.similarity_search(test_query, k=2)\n",
    "\n",
    "print(f\"\\nTest search for: '{test_query}'\")\n",
    "print(f\"Found {len(test_results)} relevant chunks:\")\n",
    "for i, doc in enumerate(test_results, 1):\n",
    "    print(f\"\\n--- Result {i} ---\")\n",
    "    print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c961051",
   "metadata": {},
   "source": [
    "## **5. Setting Up OpenAI Cloud LLM**\n",
    "\n",
    "### **Understanding Cloud LLM Benefits**\n",
    "- **GPT-4/GPT-3.5**: Advanced instruction following\n",
    "- **Reliability**: Consistent, high-quality responses\n",
    "- **No Hallucination**: Better at admitting \"I don't know\"\n",
    "- **API-Based**: No local GPU or memory requirements\n",
    "\n",
    "### **Prerequisites**\n",
    "1. OpenAI API Key (from https://platform.openai.com/api-keys)\n",
    "2. Save your key in a text file: `openai_api_key.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27dcf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OpenAI API key...\n",
      " API key loaded successfully\n",
      " OpenAI GPT-3.5-Turbo model initialized successfully!\n",
      "\n",
      "Model Configuration:\n",
      "  - Model: gpt-3.5-turbo\n",
      "  - Temperature: 0.1 (factual)\n",
      "  - Max Tokens: 512\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Read OpenAI API key from text file\n",
    "print(\"Loading OpenAI API key...\")\n",
    "\n",
    "try:\n",
    "    with open(\"openai_api_key.txt\", \"r\") as f:\n",
    "        OPENAI_API_KEY = f.read().strip()\n",
    "    print(\" API key loaded successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\" Error: openai_api_key.txt not found!\")\n",
    "    print(\"Please create a file named 'openai_api_key.txt' with your OpenAI API key\")\n",
    "    raise\n",
    "\n",
    "# Initialize OpenAI Chat Model\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=\"gpt-3.5-turbo\",  # Use \"gpt-4\" for better quality, \"gpt-3.5-turbo\" for speed\n",
    "    temperature=0.1,  # Low temperature for factual responses\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "print(\" OpenAI GPT-3.5-Turbo model initialized successfully!\")\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"  - Model: gpt-3.5-turbo\")\n",
    "print(f\"  - Temperature: 0.1 (factual)\")\n",
    "print(f\"  - Max Tokens: 512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd9840f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing OpenAI LLM...\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m      5\u001b[39m test_prompt = \u001b[33m\"\u001b[39m\u001b[33mAnswer in one sentence: What is 2+2?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m test_response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest Question: What is 2+2?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_response.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\MyData\\Training and KS\\AI Solution Enablement Program\\HandsOnLabs\\lang-chain-expirement\\.venv-langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\MyData\\Training and KS\\AI Solution Enablement Program\\HandsOnLabs\\lang-chain-expirement\\.venv-langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\MyData\\Training and KS\\AI Solution Enablement Program\\HandsOnLabs\\lang-chain-expirement\\.venv-langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\MyData\\Training and KS\\AI Solution Enablement Program\\HandsOnLabs\\lang-chain-expirement\\.venv-langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\MyData\\Training and KS\\AI Solution Enablement Program\\HandsOnLabs\\lang-chain-expirement\\.venv-langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1473\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1471\u001b[39m     _handle_openai_bad_request(e)\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.APIError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m     \u001b[43m_handle_openai_api_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1474\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1475\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\MyData\\Training and KS\\AI Solution Enablement Program\\HandsOnLabs\\lang-chain-expirement\\.venv-langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1468\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1461\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1462\u001b[39m             response,\n\u001b[32m   1463\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1464\u001b[39m             metadata=generation_info,\n\u001b[32m   1465\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1466\u001b[39m         )\n\u001b[32m   1467\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1468\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1469\u001b[39m         response = raw_response.parse()\n\u001b[32m   1470\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\MyData\\Training and KS\\AI Solution Enablement Program\\HandsOnLabs\\lang-chain-expirement\\.venv-langchain\\Lib\\site-packages\\openai\\_legacy_response.py:367\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    363\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    365\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\MyData\\Training and KS\\AI Solution Enablement Program\\HandsOnLabs\\lang-chain-expirement\\.venv-langchain\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\MyData\\Training and KS\\AI Solution Enablement Program\\HandsOnLabs\\lang-chain-expirement\\.venv-langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1204\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1157\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1158\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1159\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1201\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1202\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1203\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\MyData\\Training and KS\\AI Solution Enablement Program\\HandsOnLabs\\lang-chain-expirement\\.venv-langchain\\Lib\\site-packages\\openai\\_base_client.py:1297\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1288\u001b[39m     warnings.warn(\n\u001b[32m   1289\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1290\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1291\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1292\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1293\u001b[39m     )\n\u001b[32m   1294\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1295\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, content=content, files=to_httpx_files(files), **options\n\u001b[32m   1296\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\MyData\\Training and KS\\AI Solution Enablement Program\\HandsOnLabs\\lang-chain-expirement\\.venv-langchain\\Lib\\site-packages\\openai\\_base_client.py:1070\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1067\u001b[39m             err.response.read()\n\u001b[32m   1069\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1072\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# Test the LLM with a simple query\n",
    "print(\"Testing OpenAI LLM...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "test_prompt = \"Answer in one sentence: What is 2+2?\"\n",
    "test_response = llm.invoke(test_prompt)\n",
    "\n",
    "print(f\"Test Question: What is 2+2?\")\n",
    "print(f\"Response: {test_response.content}\")\n",
    "print()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"✓ OpenAI LLM is working! Ready for RAG chain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a67a1c7",
   "metadata": {},
   "source": [
    "## **6. Building the RAG Chain with LCEL**\n",
    "\n",
    "### **Step 6.1: Load Existing Vector Store**\n",
    "If you've already created the vector store, you can load it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d2d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Load existing vector store\n",
    "db = Chroma(\n",
    "    collection_name=\"alzheimers_knowledge_base\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"./chroma_vectorstore\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded vector store with {len(db.get()['ids'])} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401917b7",
   "metadata": {},
   "source": [
    "### **Step 6.2: Create Retriever with MMR Search**\n",
    "We'll use Maximal Marginal Relevance (MMR) for diverse retrieval.\n",
    "\n",
    "**MMR Benefits:**\n",
    "- Reduces redundancy in retrieved chunks\n",
    "- Increases diversity of information\n",
    "- Better coverage of the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6892a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever with MMR search\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",  # Maximal Marginal Relevance\n",
    "    search_kwargs={\n",
    "        \"k\": 4,  # Return top 4 chunks\n",
    "        \"fetch_k\": 10,  # Fetch 10 candidates before MMR reranking\n",
    "        \"lambda_mult\": 0.5  # Balance between relevance and diversity (0.5 = balanced)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✓ Retriever created with MMR search\")\n",
    "\n",
    "# Test retriever\n",
    "test_docs = retriever.invoke(\"What causes Alzheimer's disease?\")\n",
    "print(f\"✓ Retrieved {len(test_docs)} documents for test query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a802df23",
   "metadata": {},
   "source": [
    "### **Step 6.3: Create Prompt Template**\n",
    "We'll design a prompt that instructs the LLM to answer based only on context.\n",
    "\n",
    "**OpenAI models are excellent at following instructions, so this prompt works well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"You are an AI assistant specialized in answering questions about Alzheimer's disease.\n",
    "\n",
    "Context Information:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Answer the question based ONLY on the context provided above\n",
    "- If the answer is not in the context, respond with \"I don't know based on the provided information\"\n",
    "- Be concise and accurate\n",
    "- Do not make up information\n",
    "- Do not mention \"according to the context\" in your answer\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "print(\"✓ Prompt template created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5639b",
   "metadata": {},
   "source": [
    "### **Step 6.4: Create Helper Function**\n",
    "Format retrieved documents into a single context string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9413e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Format retrieved documents into a single string.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of retrieved documents\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted context string\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "print(\"✓ Helper function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35b6e4f",
   "metadata": {},
   "source": [
    "### **Step 6.5: Initialize Output Parser**\n",
    "Use `StrOutputParser` to parse the LLM output to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf2afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "print(\"✓ Output parser initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e31520",
   "metadata": {},
   "source": [
    "### **Step 6.6: Build the RAG Chain using LCEL**\n",
    "Now we'll assemble all components into a single RAG chain using LangChain Expression Language.\n",
    "\n",
    "**Chain Structure:**\n",
    "1. **Input**: User question\n",
    "2. **Retrieval**: Get relevant chunks (context)\n",
    "3. **Format**: Combine context and question into prompt\n",
    "4. **Generate**: OpenAI LLM generates answer\n",
    "5. **Parse**: Extract string output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a76725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Build the RAG chain using LCEL\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Retrieve and format documents\n",
    "        \"question\": RunnablePassthrough()     # Pass through the question as-is\n",
    "    }\n",
    "    | prompt_template    # Format into prompt\n",
    "    | llm                # Generate response with OpenAI\n",
    "    | output_parser      # Parse to string\n",
    ")\n",
    "\n",
    "print(\"✓ RAG chain assembled successfully!\")\n",
    "print(\"\\nChain components:\")\n",
    "print(\"  1. Retriever (MMR) → formats context\")\n",
    "print(\"  2. RunnablePassthrough → passes question\")\n",
    "print(\"  3. Prompt Template → combines context + question\")\n",
    "print(\"  4. OpenAI LLM → generates answer\")\n",
    "print(\"  5. Output Parser → extracts string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d4a07",
   "metadata": {},
   "source": [
    "## **7. Querying the System**\n",
    "\n",
    "### **Test the RAG Pipeline**\n",
    "Let's test our RAG system with various questions about Alzheimer's disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b40aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Query 1: Basic information\n",
    "query_1 = \"What is Alzheimer's disease?\"\n",
    "\n",
    "print(f\"Question: {query_1}\")\n",
    "print(\"\\nProcessing...\")\n",
    "response_1 = rag_chain.invoke(query_1)\n",
    "print(f\"\\nAnswer: {response_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Query 2: Symptoms\n",
    "query_2 = \"What are the early symptoms of Alzheimer's?\"\n",
    "\n",
    "print(f\"Question: {query_2}\")\n",
    "print(\"\\nProcessing...\")\n",
    "response_2 = rag_chain.invoke(query_2)\n",
    "print(f\"\\nAnswer: {response_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b847662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Query 3: Causes\n",
    "query_3 = \"What causes Alzheimer's disease?\"\n",
    "\n",
    "print(f\"Question: {query_3}\")\n",
    "print(\"\\nProcessing...\")\n",
    "response_3 = rag_chain.invoke(query_3)\n",
    "print(f\"\\nAnswer: {response_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e216aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Query 4: Diagnosis\n",
    "query_4 = \"How is Alzheimer's disease diagnosed?\"\n",
    "\n",
    "print(f\"Question: {query_4}\")\n",
    "print(\"\\nProcessing...\")\n",
    "response_4 = rag_chain.invoke(query_4)\n",
    "print(f\"\\nAnswer: {response_4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b23078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Query 5: Prevention\n",
    "query_5 = \"Can Alzheimer's disease be prevented?\"\n",
    "\n",
    "print(f\"Question: {query_5}\")\n",
    "print(\"\\nProcessing...\")\n",
    "response_5 = rag_chain.invoke(query_5)\n",
    "print(f\"\\nAnswer: {response_5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee4a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Query 6: Out-of-context question (should say \"I don't know\")\n",
    "query_6 = \"What is the treatment for diabetes?\"\n",
    "\n",
    "print(f\"Question: {query_6}\")\n",
    "print(\"\\nProcessing...\")\n",
    "response_6 = rag_chain.invoke(query_6)\n",
    "print(f\"\\nAnswer: {response_6}\")\n",
    "print(\"\\n✓ OpenAI should correctly say 'I don't know' for non-Alzheimer's questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6fec1b",
   "metadata": {},
   "source": [
    "# RAG System Execution Summary\n",
    "\n",
    "## Steps in Order of Execution\n",
    "\n",
    "### Step 1: Install Dependencies\n",
    "Install required packages (langchain, langchain-openai, etc.)\n",
    "\n",
    "### Step 2: Load Documents\n",
    "Load `.txt` files from `data/` folder using DirectoryLoader and TextLoader\n",
    "\n",
    "### Step 3: Split Documents into Chunks\n",
    "Use RecursiveCharacterTextSplitter (chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "### Step 4: Create Embeddings\n",
    "Initialize HuggingFaceEmbeddings with BAAI/bge-base-en-v1.5 (~400MB download)\n",
    "\n",
    "### Step 5: Create Vector Store\n",
    "Initialize Chroma with persistence and add document chunks\n",
    "\n",
    "### Step 6: Setup API Key\n",
    "Create `openai_api_key.txt` file with your OpenAI API key\n",
    "\n",
    "### Step 7: Load OpenAI LLM\n",
    "Initialize ChatOpenAI with gpt-3.5-turbo model (cloud-based)\n",
    "\n",
    "### Step 8: Create Retriever\n",
    "Create retriever from Chroma with MMR search (k=4)\n",
    "\n",
    "### Step 9: Create Prompt Template\n",
    "Define template with context and question placeholders\n",
    "\n",
    "### Step 10: Define Helper Function\n",
    "Create format_docs() to join retrieved documents\n",
    "\n",
    "### Step 11: Initialize Output Parser\n",
    "Create StrOutputParser instance\n",
    "\n",
    "### Step 12: Build RAG Chain\n",
    "Assemble chain using LCEL: retriever -> prompt -> llm -> parser\n",
    "\n",
    "### Step 13: Query the System\n",
    "Use rag_chain.invoke(question) to get answers\n",
    "\n",
    "---\n",
    "\n",
    "## Component Flow\n",
    "User Question -> Retriever -> format_docs -> Prompt -> OpenAI LLM -> Parser -> Answer\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
